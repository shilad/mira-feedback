[project]
name = "mira-grader"
version = "0.1.0"
description = "MIRA - AI-assisted grading with human review"
readme = "README.md"
requires-python = ">=3.8"
authors = [
    { name = "Shilad Sen" }
]
dependencies = [
    "PyYAML>=6.0",
    "tqdm>=4.60.0",
    "faker>=20.0.0",
    "html-to-markdown>=1.0.0",
    "pandas>=2.0.0",
    "openai>=1.0.0",
    "pydantic>=2.0.0",
    "pydantic-ai>=0.0.38",
    "flask>=3.0.0",
    "flask-cors>=4.0.0",
    "rich>=13.0.0",
    "click>=8.0.0",
    "presidio-analyzer>=2.2.0",
    "spacy>=3.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "pytest-asyncio>=0.21.0",
]

[project.scripts]
anonymize-dir = "mira.tools.dir_anonymizer.cli:main"
prep-moodle = "mira.tools.moodle_prep.cli:main"
grade-submission = "mira.tools.grading_feedback.cli:main"
grade-batch = "mira.tools.grading_feedback.batch_cli:main"
grade-review = "mira.tools.grading_review_interface.cli:main"
grade-calibrate = "mira.tools.grading_feedback.calibration.calibration_cli:main"
grade-with-claude = "mira.scripts.grade_with_claude_cli:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["mira"]

[tool.uv]
dev-dependencies = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "pytest-asyncio>=0.21.0",
    "ruff>=0.1.0",
]

[tool.ruff]
line-length = 100
target-version = "py38"

[tool.ruff.lint]
select = ["E", "F", "I"]
ignore = ["E501"]  # Line too long

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --without-slow-integration"
markers = [
    "integration_test: mark test as integration test (requires API keys)",
    "slow_integration_test: mark test as slow integration test (skipped by default)",
    "asyncio: mark test as async",
]